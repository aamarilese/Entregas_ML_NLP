{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Análisis de sentimientos y técnicas de NLP\n",
    "\n",
    "En este taller podrán poner en práctica sus conocimientos sobre las diferentes técnicas para el procesamiento de lenguaje natural. El taller está constituido por 5 puntos, en los cuales deberan seguir las intrucciones de cada numeral para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos predicción sentimientos de viajeros en Twitter\n",
    "\n",
    "En este taller se usará el conjunto de datos de sentimientos sobre distintas aerolíneas de EE.UU. provenientes de Twitter. Cada observación contiene si el sentimiento de los tweets es positivo, neutral o negativo teniendo en cuenta distintas variables como aerolínea y las razones de los sentimientos negativos (como \"retraso en el vuelo\" o \"servicio grosero\"). El objetivo es predecir el sentimiento asociado a cada tweet. Para más detalles pueden visitar el siguiente enlace: [datos](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570306133677760513</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301130888122368</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301083672813571</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301031407624196</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570300817074462722</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   airline_sentiment  airline_sentiment_confidence  \\\n",
       "tweet_id                                                             \n",
       "570306133677760513           neutral                        1.0000   \n",
       "570301130888122368          positive                        0.3486   \n",
       "570301083672813571           neutral                        0.6837   \n",
       "570301031407624196          negative                        1.0000   \n",
       "570300817074462722          negative                        1.0000   \n",
       "\n",
       "                   negativereason  negativereason_confidence         airline  \\\n",
       "tweet_id                                                                       \n",
       "570306133677760513            NaN                        NaN  Virgin America   \n",
       "570301130888122368            NaN                     0.0000  Virgin America   \n",
       "570301083672813571            NaN                        NaN  Virgin America   \n",
       "570301031407624196     Bad Flight                     0.7033  Virgin America   \n",
       "570300817074462722     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "                   airline_sentiment_gold        name negativereason_gold  \\\n",
       "tweet_id                                                                    \n",
       "570306133677760513                    NaN     cairdin                 NaN   \n",
       "570301130888122368                    NaN    jnardino                 NaN   \n",
       "570301083672813571                    NaN  yvonnalynn                 NaN   \n",
       "570301031407624196                    NaN    jnardino                 NaN   \n",
       "570300817074462722                    NaN    jnardino                 NaN   \n",
       "\n",
       "                    retweet_count  \\\n",
       "tweet_id                            \n",
       "570306133677760513              0   \n",
       "570301130888122368              0   \n",
       "570301083672813571              0   \n",
       "570301031407624196              0   \n",
       "570300817074462722              0   \n",
       "\n",
       "                                                                 text  \\\n",
       "tweet_id                                                                \n",
       "570306133677760513                @VirginAmerica What @dhepburn said.   \n",
       "570301130888122368  @VirginAmerica plus you've added commercials t...   \n",
       "570301083672813571  @VirginAmerica I didn't today... Must mean I n...   \n",
       "570301031407624196  @VirginAmerica it's really aggressive to blast...   \n",
       "570300817074462722  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "                   tweet_coord              tweet_created tweet_location  \\\n",
       "tweet_id                                                                   \n",
       "570306133677760513         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "570301130888122368         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "570301083672813571         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "570301031407624196         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "570300817074462722         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                                 user_timezone  \n",
       "tweet_id                                        \n",
       "570306133677760513  Eastern Time (US & Canada)  \n",
       "570301130888122368  Pacific Time (US & Canada)  \n",
       "570301083672813571  Central Time (US & Canada)  \n",
       "570301031407624196  Pacific Time (US & Canada)  \n",
       "570300817074462722  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura de la información de archivo .zip\n",
    "tweets = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/Tweets.zip', index_col=0)\n",
    "\n",
    "# Visualización dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresión tamaño del cojunto de datos\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis descriptivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuenta de tweets por cada sentimiento\n",
    "tweets['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline\n",
       "United            3822\n",
       "US Airways        2913\n",
       "American          2759\n",
       "Southwest         2420\n",
       "Delta             2222\n",
       "Virgin America     504\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuenta de tweets por cada aerolínea\n",
    "tweets['airline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Sentiminetos por aerolínea'}, xlabel='airline'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot con cuenta de tweets por cada aerolínea y sentimiento\n",
    "pd.crosstab(index = tweets[\"airline\"],columns = tweets[\"airline_sentiment\"]).plot(kind='bar',figsize=(10, 6),alpha=0.5,rot=0,stacked=True,title=\"Sentiminetos por aerolínea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberias y Variables de interés y predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y de variable de interés (y)\n",
    "X = tweets['text']\n",
    "y = tweets['airline_sentiment'].map({'negative':-1,'neutral':0,'positive':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en set de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Uso de CountVectorizer\n",
    "\n",
    "En la celda 1 creen un modelo de random forest con la libreria sklearn que prediga el sentimiento de los tweets usando los set de entrenamiento y test definidos anteriormente. Usen la función **CountVectorizer** y presenten el desempeño del modelo con la métrica del acurracy.\n",
    "\n",
    "Recuerden que el preprocesamiento que se haga sobre los datos de entrenamiento  (*.fit_transform()*) deben ser aplicado al set de test (*.transform()*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y predicción con Random Forest\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "rf_count = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_count.fit(X_train_count, y_train)\n",
    "y_pred_count = rf_count.predict(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest con CountVectorizer:\n",
      "{'-1': {'precision': 0.7757765596450013, 'recall': 0.9633711507293355, 'f1-score': 0.8594563331405437, 'support': 3085.0}, '0': {'precision': 0.6915077989601387, 'recall': 0.4054878048780488, 'f1-score': 0.5112107623318386, 'support': 984.0}, '1': {'precision': 0.8372641509433962, 'recall': 0.46526867627785057, 'f1-score': 0.5981465880370682, 'support': 763.0}, 'accuracy': 0.7711092715231788, 'macro avg': {'precision': 0.7681828365161788, 'recall': 0.6113758772950783, 'f1-score': 0.6562712278364835, 'support': 4832.0}, 'weighted avg': {'precision': 0.7683251051017005, 'recall': 0.7711092715231788, 'f1-score': 0.747276495145983, 'support': 4832.0}}\n",
      "Confussion Matrix Random Forest con CountVectorizer:\n",
      "[[2972   90   23]\n",
      " [ 539  399   46]\n",
      " [ 320   88  355]]\n",
      "Accuracy Random Forest con CountVectorizer: 0.7711\n"
     ]
    }
   ],
   "source": [
    "class_report_rf = classification_report(y_test, y_pred_count, output_dict=True)\n",
    "confusion_matrix_rf = confusion_matrix(y_test, y_pred_count)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_count)\n",
    "print(f\"Accuracy Random Forest con CountVectorizer:\")\n",
    "print(class_report_rf)\n",
    "print(f\"Confussion Matrix Random Forest con CountVectorizer:\")\n",
    "print(confusion_matrix_rf)\n",
    "print(f\"Accuracy Random Forest con CountVectorizer: {accuracy_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2 - Eliminación de Stopwords\n",
    "\n",
    "En la celda 2 creen un modelo de random forest con la libreria sklearn que prediga el sentimiento de los tweets usando los set de entrenamiento y test definidos anteriormente. Usen la función CountVectorizer, **eliminen stopwords** y presenten el desempeño del modelo con la métrica del acurracy.\n",
    "\n",
    "Recuerden que el preprocesamiento que se haga sobre los datos de entrenamiento  (*.fit_transform()*) deben ser aplicado al set de test (*.transform()*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3 - Lematización con verbos\n",
    "\n",
    "En la celda 3 creen un modelo de random forest con la libreria sklearn que prediga el sentimiento de los tweets usando los set de entrenamiento y test definidos anteriormente. Usen la función CountVectorizer, **lematizen el texto con verbos** y presenten el desempeño del modelo con la métrica del acurracy.\n",
    "\n",
    "Recuerden que el preprocesamiento que se haga sobre los datos de entrenamiento  (*.fit_transform()*) deben ser aplicado al set de test (*.transform()*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Multiples técnicas\n",
    "\n",
    "En la celda 4 creen un modelo de random forest con la libreria sklearn que prediga el sentimiento de los tweets usando los set de entrenamiento y test definidos anteriormente. Usen la función **CountVectorizer, eliminen stopwords, lematizen el texto con verbos** y presenten el desempeño del modelo con la métrica del acurracy.\n",
    "\n",
    "Recuerden que el preprocesamiento que se haga sobre los datos de entrenamiento  (*.fit_transform()*) deben ser aplicado al set de test (*.transform()*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que limpie el texto\n",
    "# Quitamos palabras vacías y lematizamos las palabras restantes\n",
    "lemmatizer = WordNetLemmatizer()  # Inicializamos el lematizador\n",
    "def limpiar_texto(texto):\n",
    "    stop_words = set(stopwords.words('english'))  # Lista de stopwords \n",
    "    tokens = wordpunct_tokenize(texto)  # Separamos el texto por palabras (tokens)\n",
    "    \n",
    "    # Filtramos las palabras: que no sean stopwords y que sean letras (no signos)\n",
    "    tokens_filtrados = [p for p in tokens if p.lower() not in stop_words and p.isalpha()]\n",
    "    \n",
    "    # Lematizamos usando como referencia los verbos\n",
    "    lematizadas = [lemmatizer.lemmatize(p, pos='v') for p in tokens_filtrados]\n",
    "    \n",
    "    # Unimos todo nuevamente en un texto limpio\n",
    "    return ' '.join(lematizadas)\n",
    "\n",
    "# Aplicamos esta limpieza a los datos de entrenamiento y prueba\n",
    "X_train_limpio = X_train.apply(limpiar_texto)\n",
    "X_test_limpio = X_test.apply(limpiar_texto)\n",
    "\n",
    "# Convertimos los textos a vectores usando CountVectorizer\n",
    "vectorizador = CountVectorizer()\n",
    "X_train_vec = vectorizador.fit_transform(X_train_limpio)\n",
    "X_test_vec = vectorizador.transform(X_test_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy del modelo combinado: 0.7731788079470199\n",
      "\n",
      " Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.92      0.86      3085\n",
      "           0       0.62      0.45      0.52       984\n",
      "           1       0.74      0.61      0.67       763\n",
      "\n",
      "    accuracy                           0.77      4832\n",
      "   macro avg       0.72      0.66      0.68      4832\n",
      "weighted avg       0.76      0.77      0.76      4832\n",
      "\n",
      "\n",
      " Matriz de confusión:\n",
      " [[2829  181   75]\n",
      " [ 453  445   86]\n",
      " [ 204   97  462]]\n"
     ]
    }
   ],
   "source": [
    "# Creamos y entrenamos el modelo Random Forest\n",
    "modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "modelo_rf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Hacemos predicciones y evaluamos el modelo\n",
    "predicciones = modelo_rf.predict(X_test_vec)\n",
    "\n",
    "# Métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, predicciones)\n",
    "reporte = classification_report(y_test, predicciones)\n",
    "matriz_conf = confusion_matrix(y_test, predicciones)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(\" Accuracy del modelo combinado:\", accuracy)\n",
    "print(\"\\n Reporte de clasificación:\\n\", reporte)\n",
    "print(\"\\n Matriz de confusión:\\n\", matriz_conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Comparación y análisis de resultados\n",
    "\n",
    "En la celda 5 comparen los resultados obtenidos de los diferentes modelos (random forest) y comenten las ventajas del mejor modelo y las desventajas del modelo con el menor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w75FGynzwqO3"
   },
   "source": [
    "# Informe Modelo Predictivo Popularidad de Canciones en Spotify\n",
    "<p style=\"text-align: justify;\">\n",
    "Este informe presenta el desarrollo de un modelo de aprendizaje automático, cuyo objetivo es predecir el nivel de popularidad de canciones en Spotify. A lo largo del documento se describen las etapas fundamentales del proceso, incluyendo el preprocesamiento de datos, la selección y calibración del modelo, el entrenamiento y evaluación del rendimiento del mismo. Finalmente, se presenta el procedimiento de disponibilización del modelo predictivo mediante una API.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Librerias a Importar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "# Modelado y evaluación\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Modelos base\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    BaggingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    ElasticNetCV, \n",
    "    RidgeCV, \n",
    "    LassoCV, \n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "# Preprocesamiento y selección de características\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiB84TdJwqO4"
   },
   "source": [
    "## Preprocesamiento de Datos\n",
    "<p style=\"text-align: justify;\">\n",
    "En este proyecto se usó un conjunto de datos de popularidad en canciones, donde cada observación representa una canción y se tienen variables como: duración de la canción, acusticidad y tiempo, entre otras. El objetivo del modelo que se presentará más adelante es predecir qué tan popular es la canción.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTrain_Spotify.csv')\n",
    "dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTest_Spotify.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+------------------------+-------------------+----------------------+----------------------------------------+---------------+------------+----------------+----------+-------+------------+--------+---------------+----------------+--------------------+------------+-----------+---------+------------------+---------------+--------------+\n",
      "|    |   Unnamed: 0 | track_id               | artists           | album_name           | track_name                             |   duration_ms | explicit   |   danceability |   energy |   key |   loudness |   mode |   speechiness |   acousticness |   instrumentalness |   liveness |   valence |   tempo |   time_signature | track_genre   |   popularity |\n",
      "|----+--------------+------------------------+-------------------+----------------------+----------------------------------------+---------------+------------+----------------+----------+-------+------------+--------+---------------+----------------+--------------------+------------+-----------+---------+------------------+---------------+--------------|\n",
      "|  0 |            0 | 7hUhmkALyQ8SX9mJs5XI3D | Love and Rockets  | Love and Rockets     | Motorcycle                             |        211533 | False      |          0.305 |   0.849  |     9 |    -10.795 |      1 |        0.0549 |       5.82e-05 |           0.0567   |     0.464  |    0.32   | 141.793 |                4 | goth          |           22 |\n",
      "|  1 |            1 | 5x59U89ZnjZXuNAAlc8X1u | Filippa Giordano  | Filippa Giordano     | Addio del passato - From \"La traviata\" |        196000 | False      |          0.287 |   0.19   |     7 |    -12.03  |      0 |        0.037  |       0.93     |           0.000356 |     0.0834 |    0.133  |  83.685 |                4 | opera         |           22 |\n",
      "|  2 |            2 | 70Vng5jLzoJLmeLu3ayBQq | Susumu Yokota     | Symbol               | Purple Rose Minuet                     |        216506 | False      |          0.583 |   0.509  |     1 |     -9.661 |      1 |        0.0362 |       0.777    |           0.202    |     0.115  |    0.544  |  90.459 |                3 | idm           |           37 |\n",
      "|  3 |            3 | 1cRfzLJapgtwJ61xszs37b | Franz Liszt;YUNDI | Relajación y siestas | Liebeslied (Widmung), S. 566           |        218346 | False      |          0.163 |   0.0368 |     8 |    -23.149 |      1 |        0.0472 |       0.991    |           0.899    |     0.107  |    0.0387 |  69.442 |                3 | classical     |            0 |\n",
      "|  4 |            4 | 47d5lYjbiMy0EdMRV8lRou | Scooter           | Scooter Forever      | The Darkside                           |        173160 | False      |          0.647 |   0.921  |     2 |     -7.294 |      1 |        0.185  |       0.000939 |           0.371    |     0.131  |    0.171  | 137.981 |                4 | techno        |           27 |\n",
      "+----+--------------+------------------------+-------------------+----------------------+----------------------------------------+---------------+------------+----------------+----------+-------+------------+--------+---------------+----------------+--------------------+------------+-----------+---------+------------------+---------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(dataTraining.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se valida la dimensión de los datos, los tipos de variables y se indentifica la existencia de valores faltantes y duplicados. Según los resultados, se identificaron valores duplicados y algunas variables string y categoricas. De igual forma se identifica que existen variables tipo <code>int64</code> y <code>float64</code>, los cuales podrían generan un uso importante de la memoria.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de los datos de entrenamiento: (79800, 21)\n",
      "No hay valores nulos en los datos de entrenamiento.\n",
      "Número de canciones duplicadas según Track_ID:\n",
      "13080\n",
      "Tipos de variables en la base de datos:\n",
      "[dtype('int64') dtype('O') dtype('bool') dtype('float64')]\n",
      "Variables string y categóricas en la base de datos:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79800 entries, 0 to 79799\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   track_id     79800 non-null  object\n",
      " 1   artists      79800 non-null  object\n",
      " 2   album_name   79800 non-null  object\n",
      " 3   track_name   79800 non-null  object\n",
      " 4   explicit     79800 non-null  bool  \n",
      " 5   track_genre  79800 non-null  object\n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 3.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Revisión inicial de datos\n",
    "print(\"Dimensión de los datos de entrenamiento:\", dataTraining.shape)\n",
    "\n",
    "if dataTraining.isnull().sum().sum() > 0: # Verifica si hay valores nulos\n",
    "    print(\"Valores nulos en los datos de entrenamiento:\")\n",
    "    print(dataTraining.isnull().sum())\n",
    "else:\n",
    "    print(\"No hay valores nulos en los datos de entrenamiento.\")\n",
    "\n",
    "print(\"Número de canciones duplicadas según Track_ID:\")\n",
    "print(dataTraining['track_id'].duplicated().sum())\n",
    "\n",
    "print(\"Tipos de variables en la base de datos:\")\n",
    "print(dataTraining.dtypes.unique())\n",
    "\n",
    "print(\"Variables string y categóricas en la base de datos:\")\n",
    "print(dataTraining.select_dtypes(include=['object', 'bool']).info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se realizarán algunos ajustes a la base de datos. En primer lugar, se elimnan las observaciones duplicadas según la variable <code>track_id</code>, porteriormente se optimiza el uso de memoria del dataset <code>dataTraining</code> ajustando los tipos de datos según su contenido. Este último procedimiento es relevante porque reduce significativamente el consumo de memoria, mejorando la eficiencia del procesamiento y el rendimiento del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de duplicados\n",
    "dataTraining = dataTraining.drop_duplicates(subset='track_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación tipológica de variables con el objetivo de reducir el uso de memoria\n",
    "# Se recomienda usar float32 en lugar de float64 y int32 en lugar de int64\n",
    "for col in dataTraining.columns:\n",
    "    col_type = dataTraining[col].dtype\n",
    "\n",
    "    if col_type == 'float64':\n",
    "        dataTraining[col] = dataTraining[col].astype('float32')\n",
    "    elif col_type == 'int64':\n",
    "        dataTraining[col] = dataTraining[col].astype('int32')\n",
    "    elif col_type == 'bool':\n",
    "        dataTraining[col] = dataTraining[col].astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Posteriormente, se eliminaron columnas irrelevantes, como <code>Unnamed: 0</code>, presentes en la base de datos y se crean nuevas variables derivadas de las características originales, tales como la longitud del nombre de la canción, la densidad de tempo, la interacción entre energía y bailabilidad, y una variable binaria de acusticidad. Finalmente, se realizó una selección de variables, eliminando columnas como <code>track_id</code>, <code>track_name</code>, <code>artists</code>, <code>album_name</code> y <code>track_genre</code>, para conformar el conjunto de características que serían utilizadas en el entrenamiento del modelo.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de variables string\n",
    "for col in ['Unnamed: 0']:\n",
    "    if col in dataTraining.columns: dataTraining.drop(columns=col, inplace=True)\n",
    "    if col in dataTesting.columns: dataTesting.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de variables nuevas\n",
    "# Se crean variables que pueden ser útiles para el modelo\n",
    "for df in [dataTraining, dataTesting]:\n",
    "    df['track_name_length'] = df['track_name'].apply(lambda x: len(str(x)))\n",
    "    df['tempo_density'] = df['tempo'] / df['duration_ms']\n",
    "    df['energy_danceability'] = df['energy'] * df['danceability']\n",
    "    df['acousticness_bin'] = (df['acousticness'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Selección de columnas y escalado\n",
    "drop_cols = ['track_id', 'track_name', 'artists', 'album_name', 'track_genre']\n",
    "features = dataTraining.drop(columns=drop_cols + ['popularity']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurar que no hubiera errores durante el entrenamiento, se reemplazan valores infinitos o faltantes usando la mediana:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar valores infinitos por NaN y luego llenar NaN con la mediana\n",
    "for df in [dataTraining, dataTesting]:\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(df.median(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Los datos fueron escalados utilizando <code>RobustScaler</code>. El ajuste del escalador se realizó únicamente sobre el conjunto de entrenamiento mediante el método <code>.fit_transform()</code>, y posteriormente se aplicó la transformación al conjunto de prueba utilizando <code>.transform()</code>. Esto asegura que el modelo solo utilice información disponible durante el entrenamiento y que las evaluaciones en el conjunto de prueba sean realistas..<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(dataTraining[features])\n",
    "XTesting = scaler.transform(dataTesting[features])\n",
    "y = dataTraining['popularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Finalmente, como parte del preprocesamiento de datos, se realizó una selección de variables utilizando un modelo de Extreme Gradient Boosting (XGB) como base. Esta técnica permitió reducir la dimensionalidad del conjunto de datos y mejorar la eficiencia del modelo sin afectar su capacidad predictiva. Como resultado, se seleccionaron 7 variables: 4 provenientes del conjunto de datos original y 3 generadas durante las etapas anteriores de transformación.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de características con XGBRegressor\n",
    "selector = SelectFromModel(XGBRegressor(n_estimators=100, random_state=42))\n",
    "selector.fit(X, y)\n",
    "X_sel = selector.transform(X)\n",
    "X_test_sel = selector.transform(XTesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "| Columnas Seleccionadas   |\n",
      "|--------------------------|\n",
      "| explicit                 |\n",
      "| acousticness             |\n",
      "| instrumentalness         |\n",
      "| valence                  |\n",
      "| time_signature           |\n",
      "| track_name_length        |\n",
      "| energy_danceability      |\n",
      "+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Variables predictoras\n",
    "selected_columns = dataTraining[features].columns[selector.get_support()].tolist()\n",
    "print(tabulate([[col] for col in selected_columns], headers=[\"Columnas Seleccionadas\"], tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de Datos en Entrenamiento y Prueba\n",
    "<p style=\"text-align: justify;\">\n",
    "Se dividió el conjunto de datos en dos subconjuntos: uno de entrenamiento y otro de validación, utilizando una proporción del 80 % para entrenamiento y 20 % para validación.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección y Calibración del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Selección del Modelo Predictivo y Justificación\n",
    "<p style=\"text-align: justify;\">\n",
    "A través de un proceso iterativo de prueba y error con diferentes modelos predictivos, se seleccionó el modelo de Stacking como el modelo definitivo para participar en la competencia de predicción del nivel de popularidad de las canciones en Spotify.Para la construcción del Stacking, se eligieron diversos modelos base con arquitecturas distintas, con el objetivo de aportar diversidad y minimizar el riesgo de que todos cometieran los mismos errores. Entre los modelos seleccionados se encuentran:<p>\n",
    "\n",
    "- <code>SVR</code> (Support Vector Regressor) con núcleo rbf, ideal para capturar relaciones no lineales complejas.\n",
    "- Ensambles basados en árboles como <code>RandomForestRegressor</code>, <code>GradientBoostingRegressor</code>, <code>ExtraTreesRegressor</code> y <code>BaggingRegressor</code>, conocidos por su robustez frente al sobreajuste y su buena capacidad de generalización.\n",
    "- Modelos de boosting como <code>XGBoost</code>, <code>LightGBM</code> y <code>CatBoost</code>, altamente eficientes en tareas de predicción.\n",
    "- Modelos lineales (<code>ElasticNetCV</code>, <code>RidgeCV</code>, <code>LassoCV</code>), útiles para capturar relaciones lineales y aplicar regularización que ayuda a evitar el sobreajuste.\n",
    "- <code>KNeighborsRegressor</code>, que resulta efectivo para detectar patrones locales en los datos.\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "Inicialmente, se intentó utilizar <code>GridSearchCV</code> para optimizar los hiperparámetros de cada modelo base. Sin embargo, este enfoque resultó ser demasiado costoso en tiempo y recursos computacionales, y no ofreció mejoras significativas en el desempeño respecto a configuraciones manuales basadas en experiencia previa e iteraciones manuales sobre el conjunto de datos.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "Por este motivo, se optó por emplear configuraciones predefinidas que demostraron ser eficaces, permitiendo un entrenamiento más eficiente sin comprometer el rendimiento final del modelo.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se presenta el procedimiento de calibración, entrenamiento, predicción y evaluación del desempeño del modelo seleccionado.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos base\n",
    "base_models = [\n",
    "    ('svr', SVR(kernel='rbf', C=10, epsilon=0.2)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=300, max_depth=30, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=300, max_depth=10, random_state=42)),\n",
    "    ('et', ExtraTreesRegressor(n_estimators=300, max_depth=30, random_state=42)),\n",
    "    ('bag', BaggingRegressor(n_estimators=300, max_samples=0.8, max_features=0.8, random_state=42)),\n",
    "    ('xgb', XGBRegressor(n_estimators=300, learning_rate=0.075, max_depth=10, random_state=42)),\n",
    "    ('lgbm', LGBMRegressor(n_estimators=300, learning_rate=0.075, max_depth=10, random_state=42)),\n",
    "    ('catboost', CatBoostRegressor(iterations=300, depth=10, learning_rate=0.075, random_seed=42, verbose=False)),\n",
    "    ('elasticnet', ElasticNetCV(cv=5)),\n",
    "    ('ridge', RidgeCV()),\n",
    "    ('lasso', LassoCV()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=10))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(b) Parámetros usados en los modelos base**\n",
    "Para construir el modelo de `Stacking`, se usaron varios modelos base con configuraciones diferentes. Algunos modelos comparten parámetros similares, así que aquí te dejo un resumen agrupado y explicado de forma sencilla sobre lo que hace cada uno.\n",
    "\n",
    "#### **Modelos basados en árboles (RandomForest, ExtraTrees, Bagging)**\n",
    "\n",
    "```python\n",
    "n_estimators=300, max_depth=30\n",
    "\n",
    "n_estimators: Es la cantidad de árboles que se entrenan. Al usar 300, el modelo se vuelve más estable y menos sensible al ruido.\n",
    "\n",
    "max_depth: Es la profundidad máxima de los árboles. Mientras más grande, más complejo el modelo (pero también puede sobreajustar si es muy alto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para **Bagging**, también se usa:\n",
    "\n",
    "max_samples=0.8, max_features=0.8\n",
    "\n",
    "Esto hace que cada árbol vea solo una parte de los datos y de las variables, lo que genera más variedad entre ellos y mejora el desempeño general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de boosting (XGBoost, LightGBM, CatBoost, GradientBoosting)\n",
    "\n",
    "n_estimators=300, learning_rate=0.075, max_depth=10\n",
    "\n",
    "Estos modelos aprenden de los errores que cometen paso a paso.\n",
    "\n",
    "learning_rate: Define qué tanto aprende el modelo en cada paso. Un valor bajo (como 0.075) ayuda a que el modelo generalice mejor y no se sobreentrene.\n",
    "\n",
    "max_depth o depth: Controla qué tan profundo puede ser cada árbol. Se usa 10 para permitir algo de complejidad sin exagerar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modelos lineales (RidgeCV, LassoCV, ElasticNetCV)**\n",
    "No necesitan que les pongamos parámetros manualmente porque ellos mismos los calculan por dentro con validación cruzada.\n",
    "\n",
    "Son útiles porque ayudan a mantener el modelo balanceado, aportando relaciones lineales y evitando que todo dependa de modelos más complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SVR (Support Vector Regressor)**\n",
    "\n",
    "kernel='rbf', C=10, epsilon=0.2\n",
    "\n",
    "Este modelo es útil para relaciones no lineales.\n",
    "\n",
    "C=10 hace que el modelo trate de ajustar más a los datos (menos tolerancia al error).\n",
    "\n",
    "epsilon=0.2 permite que algunos errores pequeños no se penalicen, lo que lo hace un poco más flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KNeighborsRegressor**\n",
    "\n",
    "n_neighbors=10\n",
    "\n",
    "Este modelo se basa en los 10 puntos más parecidos al que se quiere predecir.\n",
    "\n",
    "Al usar más vecinos, las predicciones se suavizan, lo cual puede ayudar cuando hay ruido en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblaje de modelos\n",
    "\n",
    "# Estrategia de validación cruzada\n",
    "cv_strategy = KFold(n_splits=15, shuffle=True, random_state=42)\n",
    "\n",
    "# Crear el modelo de apilamiento\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=XGBRegressor(n_estimators=200, learning_rate=0.075, max_depth=10, random_state=42),\n",
    "    passthrough=True,\n",
    "    n_jobs=-1,\n",
    "    cv=cv_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos un `StackingRegressor` que combina varios modelos diferentes como base, y al final un `XGBRegressor` como meta-modelo. Esto nos permite aprovechar lo mejor de cada modelo y mejorar la precisión general.\n",
    "\n",
    "La validación cruzada se hizo con `KFold` (15 divisiones) para evaluar el desempeño de forma más confiable. Además, activamos `passthrough=True` para que el meta-modelo pueda usar tanto las predicciones de los modelos base como las variables originales, lo que mejora su capacidad de aprender.\n",
    "\n",
    "Elegimos XGBoost como meta-modelo porque es muy bueno captando relaciones complejas, y funciona bien en tareas de regresión como esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de stacking con los datos transformados\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de validación\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Calcular la raíz del error cuadrático medio (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE validación local: {rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica seleccionada (RMSE) es apropiada para tareas de regresión y penaliza fuertemente los errores grandes. Se calcula en el conjunto de validación, asegurando que el resultado refleje desempeño fuera de muestra.\n",
    "\n",
    "Primero se usó train_test_split para hacer una validación rápida del modelo y ver qué tan bien predecía en una parte de los datos. Sin embargo, la evaluación más importante se hizo con validación cruzada usando KFold, porque permite probar el modelo varias veces con diferentes divisiones, lo cual da una idea más confiable de su rendimiento.\n",
    "\n",
    "Luego, antes de hacer la predicción final sobre los datos de prueba (X_test_sel), se volvió a entrenar el modelo usando todo el conjunto de entrenamiento disponible (X_sel y y), para aprovechar al máximo la información y asegurar mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluación del Desempeño**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de stacking con los datos transformados\n",
    "stacking_model.fit(X_sel, y)\n",
    "\n",
    "# Generar predicciones finales en el conjunto de prueba\n",
    "test_pred = stacking_model.predict(X_test_sel)\n",
    "\n",
    "# Evaluación del modelo\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {RMSE:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaciión del dataframe de envío\n",
    "submission = pd.DataFrame({'ID': dataTesting.index, 'popularity': test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el archivo de envío para Kaggle\n",
    "submission.to_csv('test_submission_file.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción final y exportación\n",
    "Se generan predicciones sobre el conjunto de test utilizando el modelo ensamblado. Finalmente, los resultados se guardan en un archivo CSV conforme al formato de envío de Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disponibilización del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso para asegurar la disponibilidad del modelo se estructuró en tres pasos principales:\n",
    "\n",
    "#### Construcción y almacenamiento del modelo\n",
    "Se construyó un modelo y se almacenó en un archivo .pkl.\n",
    "Debido a las limitaciones de tamaño en GitHub, no se utilizó el modelo de la competencia. En su lugar, se implementó un modelo de Random Forest con 50 muestras, permitiendo que el archivo pudiera ser subido a la plataforma.\n",
    "\n",
    "#### Creación de la función en un archivo .py\n",
    "El archivo .pkl generado fue utilizado para crear una función dentro de un archivo .py.\n",
    "Esta función fue diseñada para ser importada posteriormente, facilitando la modularidad y reutilización del código.\n",
    "\n",
    "#### Desarrollo de la API\n",
    "Finalmente, se construyó la API en otro archivo .py.\n",
    "Esta API importa la función previamente creada y configura un método GET, el cual recibe los parámetros necesarios para ejecutar la predicción del modelo de forma sencilla y estructurada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Servidor utilizado\n",
    "\n",
    "<img src=\"Imagenes_Disponibilidad/Dispo_1.png\">\n",
    "<img src=\"Imagenes_Disponibilidad/Dispo_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API: http://54.242.6.90:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados de la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6KwkVtXm8OUp2XffN5k7lY</td>\n",
       "      <td>Hillsong Worship</td>\n",
       "      <td>No Other Name</td>\n",
       "      <td>No Other Name</td>\n",
       "      <td>440247</td>\n",
       "      <td>False</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.598</td>\n",
       "      <td>7</td>\n",
       "      <td>-6.984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.00511</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>148.014</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2dp5I5MJ8bQQHDoFaNRFtX</td>\n",
       "      <td>Internal Rot</td>\n",
       "      <td>Grieving Birth</td>\n",
       "      <td>Failed Organum</td>\n",
       "      <td>93933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.997</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.586</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.00521</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>122.223</td>\n",
       "      <td>4</td>\n",
       "      <td>grindcore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id           artists      album_name      track_name  \\\n",
       "0  6KwkVtXm8OUp2XffN5k7lY  Hillsong Worship   No Other Name   No Other Name   \n",
       "1  2dp5I5MJ8bQQHDoFaNRFtX      Internal Rot  Grieving Birth  Failed Organum   \n",
       "\n",
       "   duration_ms  explicit  danceability  energy  key  loudness  mode  \\\n",
       "0       440247     False         0.369   0.598    7    -6.984     1   \n",
       "1        93933     False         0.171   0.997    7    -3.586     1   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0304       0.00511             0.000     0.176   0.0466  148.014   \n",
       "1       0.1180       0.00521             0.801     0.420   0.0294  122.223   \n",
       "\n",
       "   time_signature  track_genre  \n",
       "0               4  world-music  \n",
       "1               4    grindcore  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTesting.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrada 0:\n",
    "\n",
    "<img src=\"Imagenes_Disponibilidad/Dispo_3.png\">\n",
    "\n",
    "#### Entrada 1:\n",
    "\n",
    "<img src=\"Imagenes_Disponibilidad/Dispo_4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

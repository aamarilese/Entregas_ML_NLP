{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w75FGynzwqO3"
   },
   "source": [
    "# Informe Modelo Predictivo Popularidad de Canciones en Spotify\n",
    "<p style=\"text-align: justify;\">\n",
    "Este informe presenta el desarrollo de un modelo de aprendizaje automático, cuyo ibjetivo es predecir el nivel de popularidad de canciones en Spotify. A lo largo del documento se describen las etapas fundamentales del proceso, incluyendo el preprocesamiento de datos, la selección y calibración del modelo, el entrenamiento y evaluación del rendimiento, y la disponibilización del modelo mediante una API. El enfoque se basa en el uso de modelos de árboles de decisión y ensamblajes, implementados con Python.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Librerias a Importar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "# Modelado y evaluación\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Modelos base\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Preprocesamiento y selección de características\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiB84TdJwqO4"
   },
   "source": [
    "## Preprocesamiento de Datos\n",
    "<p style=\"text-align: justify;\">\n",
    "En este proyecto se usaró el conjunto de datos de datos de popularidad en canciones, donde cada observación representa una canción y se tienen variables como: duración de la canción, acusticidad y tempo, entre otras. El objetivo es predecir qué tan popular es la canción.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTrain_Spotify.csv')\n",
    "dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTest_Spotify.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+------------------------+-------------------+----------------------+----------------------------------------+---------------+------------+----------------+----------+-------+------------+--------+---------------+----------------+--------------------+------------+-----------+---------+------------------+---------------+--------------+\n",
      "|    |   Unnamed: 0 | track_id               | artists           | album_name           | track_name                             |   duration_ms | explicit   |   danceability |   energy |   key |   loudness |   mode |   speechiness |   acousticness |   instrumentalness |   liveness |   valence |   tempo |   time_signature | track_genre   |   popularity |\n",
      "|----+--------------+------------------------+-------------------+----------------------+----------------------------------------+---------------+------------+----------------+----------+-------+------------+--------+---------------+----------------+--------------------+------------+-----------+---------+------------------+---------------+--------------|\n",
      "|  0 |            0 | 7hUhmkALyQ8SX9mJs5XI3D | Love and Rockets  | Love and Rockets     | Motorcycle                             |        211533 | False      |          0.305 |   0.849  |     9 |    -10.795 |      1 |        0.0549 |       5.82e-05 |           0.0567   |     0.464  |    0.32   | 141.793 |                4 | goth          |           22 |\n",
      "|  1 |            1 | 5x59U89ZnjZXuNAAlc8X1u | Filippa Giordano  | Filippa Giordano     | Addio del passato - From \"La traviata\" |        196000 | False      |          0.287 |   0.19   |     7 |    -12.03  |      0 |        0.037  |       0.93     |           0.000356 |     0.0834 |    0.133  |  83.685 |                4 | opera         |           22 |\n",
      "|  2 |            2 | 70Vng5jLzoJLmeLu3ayBQq | Susumu Yokota     | Symbol               | Purple Rose Minuet                     |        216506 | False      |          0.583 |   0.509  |     1 |     -9.661 |      1 |        0.0362 |       0.777    |           0.202    |     0.115  |    0.544  |  90.459 |                3 | idm           |           37 |\n",
      "|  3 |            3 | 1cRfzLJapgtwJ61xszs37b | Franz Liszt;YUNDI | Relajación y siestas | Liebeslied (Widmung), S. 566           |        218346 | False      |          0.163 |   0.0368 |     8 |    -23.149 |      1 |        0.0472 |       0.991    |           0.899    |     0.107  |    0.0387 |  69.442 |                3 | classical     |            0 |\n",
      "|  4 |            4 | 47d5lYjbiMy0EdMRV8lRou | Scooter           | Scooter Forever      | The Darkside                           |        173160 | False      |          0.647 |   0.921  |     2 |     -7.294 |      1 |        0.185  |       0.000939 |           0.371    |     0.131  |    0.171  | 137.981 |                4 | techno        |           27 |\n",
      "+----+--------------+------------------------+-------------------+----------------------+----------------------------------------+---------------+------------+----------------+----------+-------+------------+--------+---------------+----------------+--------------------+------------+-----------+---------+------------------+---------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(dataTraining.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se valida la dimensión de los datos, los tipos de variables y se indentifica la existencia de valores faltantes y duplicados. Según los resultados, se identificaron valores duplicados y algunas variables string y categoricas. De igual forma se identifica que existen variables tipo <code>int64</code> y <code>float64</code>, los cuales podrían generan un uso importante de la memoria.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de los datos de entrenamiento: (79800, 21)\n",
      "\n",
      "No hay valores nulos en los datos de entrenamiento.\n",
      "\n",
      "Número de canciones duplicadas según Track_ID:\n",
      "13080\n",
      "\n",
      "Tipos de variables en la base de datos:\n",
      "[dtype('int64') dtype('O') dtype('bool') dtype('float64')]\n",
      "\n",
      "Variables string y categóricas en la base de datos:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79800 entries, 0 to 79799\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   track_id     79800 non-null  object\n",
      " 1   artists      79800 non-null  object\n",
      " 2   album_name   79800 non-null  object\n",
      " 3   track_name   79800 non-null  object\n",
      " 4   explicit     79800 non-null  bool  \n",
      " 5   track_genre  79800 non-null  object\n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 3.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Revisión inicial de datos\n",
    "print(\"Dimensión de los datos de entrenamiento:\", dataTraining.shape)\n",
    "\n",
    "if dataTraining.isnull().sum().sum() > 0: # Verifica si hay valores nulos\n",
    "    print(\"\\nValores nulos en los datos de entrenamiento:\")\n",
    "    print(dataTraining.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nNo hay valores nulos en los datos de entrenamiento.\")\n",
    "\n",
    "print(\"\\nNúmero de canciones duplicadas según Track_ID:\")\n",
    "print(dataTraining['track_id'].duplicated().sum())\n",
    "\n",
    "print(\"\\nTipos de variables en la base de datos:\")\n",
    "print(dataTraining.dtypes.unique())\n",
    "\n",
    "print(\"\\nVariables string y categóricas en la base de datos:\")\n",
    "print(dataTraining.select_dtypes(include=['object', 'bool']).info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se realizarán algunos ajustes a la base de datos. En primer lugar se optimiza el uso de memoria del dataset <code>dataTraining</code> ajustando los tipos de datos según su contenido. Este procedimiento es relevante porque reduce significativamente el consumo de memoria, mejorando la eficiencia del procesamiento y el rendimiento del modelo. Posteriormente, se eliminan las columnas que no se usarán y crearán nuevas variables que serán incluidas en el modelo.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de duplicados\n",
    "dataTraining = dataTraining.drop_duplicates(subset='track_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación tipológica de variables con el objetivo de reducir el uso de memoria\n",
    "# Se recomienda usar float32 en lugar de float64 y int32 en lugar de int64\n",
    "for col in dataTraining.columns:\n",
    "    col_type = dataTraining[col].dtype\n",
    "\n",
    "    if col_type == 'float64':\n",
    "        dataTraining[col] = dataTraining[col].astype('float32')\n",
    "    elif col_type == 'int64':\n",
    "        dataTraining[col] = dataTraining[col].astype('int32')\n",
    "    elif col_type == 'bool':\n",
    "        dataTraining[col] = dataTraining[col].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de variables nuevas\n",
    "# Se crean variables que pueden ser útiles para el modelo\n",
    "for df in [dataTraining, dataTesting]:\n",
    "    df['track_name_length'] = df['track_name'].apply(lambda x: len(str(x)))\n",
    "    df['tempo_density'] = df['tempo'] / df['duration_ms']\n",
    "    df['energy_danceability'] = df['energy'] * df['danceability']\n",
    "    df['acousticness_bin'] = (df['acousticness'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de variables string\n",
    "for col in ['Unnamed: 0']:\n",
    "    if col in dataTraining.columns: dataTraining.drop(columns=col, inplace=True)\n",
    "    if col in dataTesting.columns: dataTesting.drop(columns=col, inplace=True)\n",
    "\n",
    "drop_cols = ['track_id','artists','album_name','track_name', 'track_genre']\n",
    "X = dataTraining.drop(columns=drop_cols + ['popularity'])\n",
    "y = dataTraining['popularity']\n",
    "XTesting = dataTesting.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Adicionalmente, como parte del pre procesamiento de datos se realiza una selección de variables utilizando un modelo de Ranfom Forest como base. Este modelo perimite seleccionar las características según su nivel de relevancia. De acuerdo con los resultados obtenidos se seleccionaron 13 variables de las cuales 10 vienen de la base de datos original y 3 fueron creadas en pasos previos.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de variables usando Random Forest\n",
    "selector = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "selector.fit(X, y)\n",
    "X_sel = selector.transform(X)\n",
    "X_test_sel = selector.transform(XTesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "| Columnas Seleccionadas   |\n",
      "|--------------------------|\n",
      "| duration_ms              |\n",
      "| danceability             |\n",
      "| energy                   |\n",
      "| loudness                 |\n",
      "| speechiness              |\n",
      "| acousticness             |\n",
      "| instrumentalness         |\n",
      "| liveness                 |\n",
      "| valence                  |\n",
      "| tempo                    |\n",
      "| track_name_length        |\n",
      "| tempo_density            |\n",
      "| energy_danceability      |\n",
      "+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Variables predictoras\n",
    "selected_columns = X.columns[selector.get_support()].tolist()\n",
    "print(tabulate([[col] for col in selected_columns], headers=[\"Columnas Seleccionadas\"], tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Estadísticas Descriptivas\n",
    "<p style=\"text-align: justify;\">\n",
    "En esta sección se realiza un análisis descriptivo de la base de datos. Se presentan las principales estadísticas descriptivas, así como histogramas, correlaciones y visualización de valores atípicos.<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización datos de entrenamiento\n",
    "dataTraining.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estilos de los gráficos\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Histograma de variables numéricas\n",
    "dataTraining.select_dtypes(include=np.number).hist(bins=30, figsize=(18, 12), color='skyblue', edgecolor='black')\n",
    "plt.suptitle(\"Distribución de variables numéricas\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mapa de calor de correlación\n",
    "plt.figure(figsize=(14, 10))\n",
    "corr = dataTraining.select_dtypes(include=np.number).corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title(\"Matriz de Correlación\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Boxplots para identificar outliers ---\n",
    "for col in dataTraining.select_dtypes(include=np.number).columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(x=dataTraining[col], color='lightgreen')\n",
    "    plt.title(f\"Boxplot: {col}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categóricas\n",
    "cat_cols = dataTraining.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# Visualizar la cantidad de valores únicos por variable categórica\n",
    "print(\"Valores únicos por variable categórica:\")\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {dataTraining[col].nunique()} valores únicos\")\n",
    "\n",
    "# Distribución de frecuencia de las principales categorías\n",
    "for col in cat_cols:\n",
    "    if dataTraining[col].nunique() <= 50:  # Limita el análisis a columnas con un número manejable de categorías\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        dataTraining[col].value_counts().head(20).plot(kind='bar', color='cornflowerblue')\n",
    "        plt.title(f\"Top 20 valores de {col}\")\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        plt.xlabel(col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "# Popularidad promedio por categoría\n",
    "for col in cat_cols:\n",
    "    if 'popularity' in dataTraining.columns and dataTraining[col].nunique() <= 50:\n",
    "        popularity_by_cat = dataTraining.groupby(col)['popularity'].mean().sort_values(ascending=False).head(20)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x=popularity_by_cat.values, y=popularity_by_cat.index, palette=\"viridis\")\n",
    "        plt.title(f\"Popularidad promedio por {col} (Top 20)\")\n",
    "        plt.xlabel(\"Popularidad Promedio\")\n",
    "        plt.ylabel(col)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'track_genre' y calcular las medias de las columnas seleccionadas\n",
    "perfil_generos = dataTraining.groupby('track_genre')[dataTraining.select_dtypes(include=np.number).columns].mean().round(2).sort_values('popularity', ascending=False)\n",
    "\n",
    "# Mostrar el perfil\n",
    "print(perfil_generos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de Datos en Entrenamiento y Prueba\n",
    "<p style=\"text-align: justify;\">\n",
    "Se dividió el conjunto de datos en dos subconjuntos: uno de entrenamiento y otro de validación, utilizando una proporción del 80 % para entrenamiento y 20 % para validación.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección y Calibración del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Selección del Modelo Predictivo\n",
    "<p style=\"text-align: justify;\">\n",
    "A partir de un proceso iterativo de prueba y error con diferentes modelos predictivos, se eligió el modelo de Satcking como el modelo definitivo para participar en la competencia de predicción del nivel de popularidad de las cacniones en Spotify. A continuación se presentarán el procedimiento de calibración, entrenamiento, predicción y evaluación del desempeño del mismo.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Calibración de Modelos Individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrar Random Forest\n",
    "rf_params = {'n_estimators': [100, 200], 'max_depth': [20, 30]}\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=3)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrar Gradient Boosting\n",
    "gb_params = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.075], 'max_depth': [3, 5]}\n",
    "gb_grid = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_params, cv=3)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "best_gb = gb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrar XGBoost\n",
    "xgb_params = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.075], 'max_depth': [3, 5]}\n",
    "xgb_grid = GridSearchCV(XGBRegressor(random_state=42), xgb_params, cv=3)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "best_xgb = xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Entrenamiento del Modelo Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos base\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=200, learning_rate=0.075, max_depth=5, random_state=42)),\n",
    "    ('xgb', XGBRegressor(n_estimators=200, learning_rate=0.075, max_depth=5, random_state=42))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo de stacking\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=GradientBoostingRegressor(n_estimators=100, learning_rate=0.05, max_depth=3),\n",
    "    passthrough=True,\n",
    "    n_jobs=-1,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de stacking con los datos transformados\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de validación\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Calcular la raíz del error cuadrático medio (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE validación local: {rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de stacking con los datos transformados\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Generar predicciones finales en el conjunto de prueba\n",
    "test_pred = stacking_model.predict(X_test_sel)\n",
    "submission = pd.DataFrame({'ID': dataTesting.index, 'popularity': test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el archivo de envío para Kaggle\n",
    "submission.to_csv('test_submission_file.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Evaluación del Desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {RMSE:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disponibilización del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

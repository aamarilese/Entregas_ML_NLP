{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w75FGynzwqO3"
   },
   "source": [
    "# Informe Modelo Predictivo Popularidad de Canciones en Spotify\n",
    "\n",
    "Integrantes:<br>\n",
    "- Miguel Mateo Sandoval Torres\n",
    "- Diego Dayan Niño Perez\n",
    "- Camilo Andres Florez Esquivel\n",
    "- Andrea Amariles Escobar\n",
    "\n",
    "Curso:<br>\n",
    "Machine Learning y Procesamiento de Lenguaje Natural<br>\n",
    "\n",
    "Fecha: <br>\n",
    "Abril 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción <br>\n",
    "<p style=\"text-align: justify;\">\n",
    "Este informe presenta el desarrollo de un modelo de aprendizaje automático, cuyo objetivo es predecir el nivel de popularidad de canciones en Spotify. A lo largo del documento se describen las etapas fundamentales del proceso, incluyendo el preprocesamiento de datos, la selección y calibración del modelo, el entrenamiento y evaluación del rendimiento del mismo. Finalmente, se presenta el procedimiento de disponibilización del modelo predictivo mediante una API.<p>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 19,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "## Librerias a Importar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "# Modelado y evaluación\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Modelos base\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    BaggingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    ElasticNetCV, \n",
    "    RidgeCV, \n",
    "    LassoCV, \n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "# Preprocesamiento y selección de características\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiB84TdJwqO4"
   },
   "source": [
    "## Preprocesamiento de Datos\n",
    "<p style=\"text-align: justify;\">\n",
    "En este proyecto se usó un conjunto de datos de popularidad en canciones, donde cada observación representa una canción y se tienen variables como: duración de la canción, acusticidad y tiempo, entre otras. El objetivo del modelo que se presentará más adelante es predecir qué tan popular es la canción.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTrain_Spotify.csv')\n",
    "dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTest_Spotify.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(dataTraining.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se valida la dimensión de los datos, los tipos de variables y se indentifica la existencia de valores faltantes y duplicados. Según los resultados, se identificaron valores duplicados y algunas variables string y categoricas. De igual forma se identifica que existen variables tipo <code>int64</code> y <code>float64</code>, los cuales podrían generan un uso importante de la memoria.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisión inicial de datos\n",
    "print(\"Dimensión de los datos de entrenamiento:\", dataTraining.shape)\n",
    "\n",
    "if dataTraining.isnull().sum().sum() > 0: # Verifica si hay valores nulos\n",
    "    print(\"Valores nulos en los datos de entrenamiento:\")\n",
    "    print(dataTraining.isnull().sum())\n",
    "else:\n",
    "    print(\"No hay valores nulos en los datos de entrenamiento.\")\n",
    "\n",
    "print(\"Número de canciones duplicadas según Track_ID:\")\n",
    "print(dataTraining['track_id'].duplicated().sum())\n",
    "\n",
    "print(\"Tipos de variables en la base de datos:\")\n",
    "print(dataTraining.dtypes.unique())\n",
    "\n",
    "print(\"Variables string y categóricas en la base de datos:\")\n",
    "print(dataTraining.select_dtypes(include=['object', 'bool']).info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se realizarán algunos ajustes a la base de datos. En primer lugar, se elimnan las observaciones duplicadas según la variable <code>track_id</code>, porteriormente se optimiza el uso de memoria del dataset <code>dataTraining</code> ajustando los tipos de datos según su contenido. Este último procedimiento es relevante porque reduce significativamente el consumo de memoria, mejorando la eficiencia del procesamiento y el rendimiento del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de duplicados\n",
    "dataTraining = dataTraining.drop_duplicates(subset='track_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación tipológica de variables con el objetivo de reducir el uso de memoria\n",
    "# Se recomienda usar float32 en lugar de float64 y int32 en lugar de int64\n",
    "for col in dataTraining.columns:\n",
    "    col_type = dataTraining[col].dtype\n",
    "\n",
    "    if col_type == 'float64':\n",
    "        dataTraining[col] = dataTraining[col].astype('float32')\n",
    "    elif col_type == 'int64':\n",
    "        dataTraining[col] = dataTraining[col].astype('int32')\n",
    "    elif col_type == 'bool':\n",
    "        dataTraining[col] = dataTraining[col].astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Posteriormente, se eliminaron columnas irrelevantes, como <code>Unnamed: 0</code>, presentes en la base de datos y se crean nuevas variables derivadas de las características originales, tales como la longitud del nombre de la canción, la densidad de tempo, la interacción entre energía y bailabilidad, y una variable binaria de acusticidad. Finalmente, se realizó una selección de variables, eliminando columnas como <code>track_id</code>, <code>track_name</code>, <code>artists</code>, <code>album_name</code> y <code>track_genre</code>, para conformar el conjunto de características que serían utilizadas en el entrenamiento del modelo.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de variables string\n",
    "for col in ['Unnamed: 0']:\n",
    "    if col in dataTraining.columns: dataTraining.drop(columns=col, inplace=True)\n",
    "    if col in dataTesting.columns: dataTesting.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de variables nuevas\n",
    "# Se crean variables que pueden ser útiles para el modelo\n",
    "for df in [dataTraining, dataTesting]:\n",
    "    df['track_name_length'] = df['track_name'].apply(lambda x: len(str(x)))\n",
    "    df['tempo_density'] = df['tempo'] / df['duration_ms']\n",
    "    df['energy_danceability'] = df['energy'] * df['danceability']\n",
    "    df['acousticness_bin'] = (df['acousticness'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Selección de columnas y escalado\n",
    "drop_cols = ['track_id', 'track_name', 'artists', 'album_name', 'track_genre']\n",
    "features = dataTraining.drop(columns=drop_cols + ['popularity']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurar que no hubiera errores durante el entrenamiento, se reemplazan valores infinitos o faltantes usando la mediana:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar valores infinitos por NaN y luego llenar NaN con la mediana\n",
    "for df in [dataTraining, dataTesting]:\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(df.median(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Los datos fueron escalados utilizando <code>RobustScaler</code>. El ajuste del escalador se realizó únicamente sobre el conjunto de entrenamiento mediante el método <code>.fit_transform()</code>, y posteriormente se aplicó la transformación al conjunto de prueba utilizando <code>.transform()</code>. Esto asegura que el modelo solo utilice información disponible durante el entrenamiento y que las evaluaciones en el conjunto de prueba sean realistas..<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(dataTraining[features])\n",
    "XTesting = scaler.transform(dataTesting[features])\n",
    "y = dataTraining['popularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Finalmente, como parte del preprocesamiento de datos, se realizó una selección de variables utilizando un modelo de Extreme Gradient Boosting (XGB) como base. Esta técnica permitió reducir la dimensionalidad del conjunto de datos y mejorar la eficiencia del modelo sin afectar su capacidad predictiva. Como resultado, se seleccionaron 7 variables: 4 provenientes del conjunto de datos original y 3 generadas durante las etapas anteriores de transformación.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de características con XGBRegressor\n",
    "selector = SelectFromModel(XGBRegressor(n_estimators=100, random_state=42))\n",
    "selector.fit(X, y)\n",
    "X_sel = selector.transform(X)\n",
    "X_test_sel = selector.transform(XTesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "| Columnas Seleccionadas   |\n",
      "|--------------------------|\n",
      "| explicit                 |\n",
      "| acousticness             |\n",
      "| instrumentalness         |\n",
      "| valence                  |\n",
      "| time_signature           |\n",
      "| track_name_length        |\n",
      "| energy_danceability      |\n",
      "+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Variables predictoras\n",
    "selected_columns = dataTraining[features].columns[selector.get_support()].tolist()\n",
    "print(tabulate([[col] for col in selected_columns], headers=[\"Columnas Seleccionadas\"], tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de Datos en Entrenamiento y Prueba\n",
    "<p style=\"text-align: justify;\">\n",
    "Se dividió el conjunto de datos en dos subconjuntos: uno de entrenamiento y otro de validación, utilizando una proporción del 80 % para entrenamiento y 20 % para validación.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección y Calibración del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Selección del Modelo Predictivo y Justificación\n",
    "<p style=\"text-align: justify;\">\n",
    "A través de un proceso iterativo de prueba y error con diferentes modelos predictivos, se seleccionó el modelo de Stacking como el modelo definitivo para participar en la competencia de predicción del nivel de popularidad de las canciones en Spotify.Para la construcción del Stacking, se eligieron diversos modelos base con arquitecturas distintas, con el objetivo de aportar diversidad y minimizar el riesgo de que todos cometieran los mismos errores. Entre los modelos seleccionados se encuentran:<p>\n",
    "\n",
    "- <code>SVR</code> (Support Vector Regressor) con núcleo rbf, ideal para capturar relaciones no lineales complejas.\n",
    "- Ensambles basados en árboles como <code>RandomForestRegressor</code>, <code>GradientBoostingRegressor</code>, <code>ExtraTreesRegressor</code> y <code>BaggingRegressor</code>, conocidos por su robustez frente al sobreajuste y su buena capacidad de generalización.\n",
    "- Modelos de boosting como <code>XGBoost</code>, <code>LightGBM</code> y <code>CatBoost</code>, altamente eficientes en tareas de predicción.\n",
    "- Modelos lineales (<code>ElasticNetCV</code>, <code>RidgeCV</code>, <code>LassoCV</code>), útiles para capturar relaciones lineales y aplicar regularización que ayuda a evitar el sobreajuste.\n",
    "- <code>KNeighborsRegressor</code>, que resulta efectivo para detectar patrones locales en los datos.\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "Inicialmente, se intentó utilizar <code>GridSearchCV</code> para optimizar los hiperparámetros de cada modelo base. Sin embargo, este enfoque resultó ser demasiado costoso en tiempo y recursos computacionales, y no ofreció mejoras significativas en el desempeño respecto a configuraciones manuales basadas en experiencia previa e iteraciones manuales sobre el conjunto de datos.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "Por este motivo, se optó por emplear configuraciones predefinidas que demostraron ser eficaces, permitiendo un entrenamiento más eficiente sin comprometer el rendimiento final del modelo.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "A continuación, se presenta el procedimiento de calibración, entrenamiento, predicción y evaluación del desempeño del modelo seleccionado.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos base\n",
    "base_models = [\n",
    "    ('svr', SVR(kernel='rbf', C=10, epsilon=0.2)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=300, max_depth=30, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=300, max_depth=10, random_state=42)),\n",
    "    ('et', ExtraTreesRegressor(n_estimators=300, max_depth=30, random_state=42)),\n",
    "    ('bag', BaggingRegressor(n_estimators=300, max_samples=0.8, max_features=0.8, random_state=42)),\n",
    "    ('xgb', XGBRegressor(n_estimators=300, learning_rate=0.075, max_depth=10, random_state=42)),\n",
    "    ('lgbm', LGBMRegressor(n_estimators=300, learning_rate=0.075, max_depth=10, random_state=42)),\n",
    "    ('catboost', CatBoostRegressor(iterations=300, depth=10, learning_rate=0.075, random_seed=42, verbose=False)),\n",
    "    ('elasticnet', ElasticNetCV(cv=5)),\n",
    "    ('ridge', RidgeCV()),\n",
    "    ('lasso', LassoCV()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=10))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Parámetros usados en los modelos base\n",
    "<p style=\"text-align: justify;\">\n",
    "Para la construcción del modelo de Stacking se utilizaron varios modelos base configurados inidividualmente. Dado que algunos modelos comparten hiperparámetros similares, a continuación se presenta un resumen agrupado que describe de manera sencilla las principales configuraciones utilizadas:<p>\n",
    "\n",
    "**Modelos basados en árboles RandomForest, ExtraTrees, Bagging**<br>\n",
    "<code>n_estimators=300</code>, <code>max_depth=30</code>\n",
    "\n",
    "- n_estimators: Define la cantidad de árboles que se entrenan en el ensamblaje.\n",
    "- max_depth: Controla la profundidad máxima de cada árbol. Aumentar este valor permite capturar relaciones más complejas, aunque profundidades muy grandes pueden llevar al sobreajuste.\n",
    "- Adicionalmente, en Bagging también se usa <code>max_samples=0.8</code> y <code>max_features=0.8</code>.\n",
    "\n",
    "**Modelos de boosting (XGBoost, LightGBM, CatBoost, GradientBoosting)**<br>\n",
    "<code>n_estimators=300</code>, <code>learning_rate=0.075</code>, <code>max_depth=10</code>\n",
    "\n",
    "Estos modelos construyen árboles de manera secuencial, donde cada nuevo árbol intenta corregir los errores cometidos por los árboles anteriores.\n",
    "\n",
    "- learning_rate: Controla la velocidad de aprendizaje del modelo en cada iteración. Un valor bajo, como 0.075, permite que el modelo ajuste de forma más gradual, favoreciendo una mejor generalización y reduciendo el riesgo de sobreajuste.\n",
    "- max_depth o depth: Determina la profundidad máxima de cada árbol. Se fijó en 10 para capturar relaciones de cierta complejidad sin llegar a niveles que puedan inducir sobreajuste.\n",
    "\n",
    "**Modelos lineales (RidgeCV, LassoCV, ElasticNetCV)**\n",
    "<p style=\"text-align: justify;\">\n",
    "No requieren la asignación manual de hiperparámetros, ya que calculan automáticamente sus configuraciones internas mediante validación cruzada. Estos modelos son especialmente útiles para mantener el equilibrio del conjunto de modelos base, ya que capturan relaciones lineales y contribuyen a evitar que el ensamble dependa exclusivamente de modelos más complejos.<p>\n",
    "\n",
    "**SVR (Support Vector Regressor)**<br>\n",
    "<code>kernel='rbf'</code>, <code>C=10</code>, <code>epsilon=0.2</code>\n",
    "\n",
    "El modelo SVR es particularmente útil para capturar relaciones no lineales en los datos.\n",
    "\n",
    "- C=10: Controla el nivel de penalización por errores; un valor alto como 10 hace que el modelo intente ajustarse más estrechamente a los datos, tolerando menos desviaciones.\n",
    "- epsilon=0.2: Define un margen de tolerancia dentro del cual los errores no son penalizados, permitiendo una mayor flexibilidad y estabilidad frente a pequeñas fluctuaciones en los datos.\n",
    "\n",
    "**KNeighborsRegressor**<br>\n",
    "<code>n_neighbors=10</code>\n",
    "<p style=\"text-align: justify;\">\n",
    "Este modelo realiza las predicciones basándose en los 10 vecinos más cercanos al punto que se desea estimar. Utilizar un número mayor de vecinos contribuye a suavizar las predicciones, lo que resulta beneficioso en escenarios donde los datos presentan variabilidad o ruido.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la estrategia de validación cruzada\n",
    "cv_strategy = KFold(n_splits=15, shuffle=True, random_state=42)\n",
    "\n",
    "# Crear el modelo de apilamiento\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=XGBRegressor(n_estimators=200, learning_rate=0.075, max_depth=10, random_state=42),\n",
    "    passthrough=True,\n",
    "    n_jobs=-1,\n",
    "    cv=cv_strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Se utilizó un <code>StackingRegressor</code> que combina varios modelos base con diferentes arquitecturas, utilizando un <code>XGBRegressor</code> como meta-modelo. Esta estrategia permite aprovechar las fortalezas individuales de cada modelo, mejorando así la precisión general de las predicciones.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "La validación cruzada se realizó mediante <code>KFold</code>  con 15 divisiones, lo que proporciona una evaluación más robusta y confiable del desempeño del modelo. Además, se activó el parámetro <code>passthrough=True</code>, permitiendo que el meta-modelo acceda tanto a las predicciones de los modelos base como a las variables originales, incrementando su capacidad de aprendizaje.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "Se eligió <code>XGBoost</code> como meta-modelo debido a su capacidad para capturar relaciones complejas en los datos y su excelente desempeño en tareas de regresión.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de stacking con los datos transformados\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de validación\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Calcular la raíz del error cuadrático medio (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE validación local: {rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "La métrica seleccionada (RMSE) es apropiada para tareas de regresión y penaliza fuertemente los errores grandes. Se calcula en el conjunto de validación, asegurando que el resultado refleje desempeño fuera de muestra.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "Primero se usó train_test_split para hacer una validación rápida del modelo y ver qué tan bien predecía en una parte de los datos. Sin embargo, la evaluación más importante se hizo con validación cruzada usando KFold, porque permite probar el modelo varias veces con diferentes divisiones, lo cual da una idea más confiable de su rendimiento.<p>\n",
    "<p style=\"text-align: justify;\">\n",
    "Luego, antes de hacer la predicción final sobre los datos de prueba (X_test_sel), se volvió a entrenar el modelo usando todo el conjunto de entrenamiento disponible (X_sel y y), para aprovechar al máximo la información y asegurar mejores resultados. El RMSE el cual obtuvo un valor de 11.15 usando la base de entrenamiento <code>DataTraining</code>.<p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción en Muestra de Evaluación: DataTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de stacking con los datos transformados\n",
    "stacking_model.fit(X_sel, y)\n",
    "\n",
    "# Generar predicciones finales en el conjunto de prueba\n",
    "test_pred = stacking_model.predict(X_test_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaciión del dataframe de envío\n",
    "submission = pd.DataFrame({'ID': dataTesting.index, 'popularity': test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el archivo de envío para Kaggle\n",
    "submission.to_csv('test_submission_file.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Se generaron predicciones sobre el conjunto de test utilizando el modelo descrito anteriormente y se guardan los resultados en un archivo CSV conforme al formato de envío de Kaggle. Según los resultados obtenidos el modelo presentó un RMSE en Kaggle de 10.18.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disponibilización del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso para asegurar la disponibilidad del modelo se estructuró en tres pasos:\n",
    "\n",
    "**Construcción y almacenamiento del modelo:**<br>\n",
    "Se construyó un modelo y se almacenó en un archivo .pkl.\n",
    "Debido a las limitaciones de tamaño en GitHub, no se utilizó el modelo de la competencia. En su lugar, se implementó un modelo de Random Forest con 50 muestras, permitiendo que el archivo pudiera ser subido a la plataforma.\n",
    "\n",
    "**Creación de la función en un archivo .py:**<br>\n",
    "El archivo .pkl generado fue utilizado para crear una función dentro de un archivo .py.\n",
    "Esta función fue diseñada para ser importada posteriormente, facilitando la modularidad y reutilización del código.\n",
    "\n",
    "**Desarrollo de la API:**<br>\n",
    "Finalmente, se construyó la API en otro archivo .py.\n",
    "Esta API importa la función previamente creada y configura un método GET, el cual recibe los parámetros necesarios para ejecutar la predicción del modelo de forma sencilla y estructurada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Servidor Utilizado\n",
    "![Descripción de la imagen](https://github.com/aamarilese/Entregas_ML_NLP/blob/main/Semana%203/Imagenes_Disponibilidad/Dispo_1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API: http://54.242.6.90:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados de la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTesting.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrada 0:\n",
    "\n",
    "![Descripción de la imagen](https://github.com/aamarilese/Entregas_ML_NLP/blob/main/Semana%203/Imagenes_Disponibilidad/Dispo_3.png?raw=true)\n",
    "\n",
    "#### Entrada 1:\n",
    "\n",
    "![Descripción de la imagen](https://github.com/aamarilese/Entregas_ML_NLP/blob/main/Semana%203/Imagenes_Disponibilidad/Dispo_4.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto, se desarrolló un modelo de aprendizaje automático para predecir la popularidad de canciones en Spotify. A lo largo del notebook, se implementaron diversas etapas clave. A continuación, se resumen los puntos más destacados:\n",
    "\n",
    "1. Preprocesamiento de Datos:\n",
    "   - Se realizó una limpieza de los datos, eliminando duplicados y optimizando el uso de memoria mediante la conversión de tipos de datos.\n",
    "   - Se crearon nuevas variables como la longitud del nombre de la canción y la densidad de tempo, para enriquecer el conjunto de características.\n",
    "   - Se manejaron valores faltantes e infinitos reemplazándolos con la mediana, asegurando la integridad de los datos.\n",
    "2. Selección de Características:\n",
    "   - Se utilizó un modelo de XGBoost para identificar las características más relevantes, reduciendo la dimensionalidad del conjunto de datos sin comporo.\n",
    "3. Entrenamiento del Modelo:\n",
    "   - Se implementó un modelo de Stacking Regressor, combinando múltiples modelos base (Random Forest, Gradient Boosting, XGBoost, entre otros) con un meta-modelo de XGBoost.\n",
    "   - La validación cruzada con KFold permitió evaluar el desempeño del modelo de manera robusta.\n",
    "4. Evaluación del Modelo:\n",
    "   - La métrica seleccionada, RMSE, demostró ser adecuada para medir el error en las predicciones de popularidad.\n",
    "   - El modelo final mostró un buen desempeño en el conjunto de validación, reflejando su capacidad para capturar patrones en los datos.\n",
    "5. Disponibilización del Modelo:\n",
    "   - Se almacenó el modelo entrenado en un archivo .pkl y se desarrolló una API para facilitar su uso en aplicaciones externas.\n",
    "   - La API permite realizar predicciones de manera sencilla, asegurando la accesibilidad del modelo para usuarios finales.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

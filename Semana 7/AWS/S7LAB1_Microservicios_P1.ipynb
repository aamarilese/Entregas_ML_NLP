{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disponibilización de modelos\n",
    "\n",
    "En este notebook aprenderá a guardar un modelo y a disponibilizarlo como una API con la librería Flask. Una API (interfaz de programación de aplicaciones) es un conjunto de definiciones y protocolos que permiten que servicios, en este caso modelos, retornen resultados y respuestas sin necesidad de saber cómo están implementados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones Generales:\n",
    "\n",
    "#### Este notebook va a se modificado para realizar los pasos necesarios para guardar un modelo para luego disponibilizarlo como una API.\n",
    "\n",
    "Este notebook esta compuesto por dos secciones. En la primera secciónn, usted beberá entrenar y guardar (exportar) un modelo de random forest para predecir si una URL es phishing (fraudulenta) o no. En la segunda parte, usará el modelo entrenado y lo disponibilizara usando la libreria *Flask*. En el siguente paper puede conocer más detalles de la base de datos que usaremos y del problema: *A. Correa Bahnsen, E. C. Bohorquez, S. Villegas, J. Vargas, and F. A. Gonzalez, “Classifying phishing urls using recurrent neural networks,” in Electronic Crime Research (eCrime), 2017 APWG Symposium on. IEEE, 2017, pp. 1–8*. https://albahnsen.files.wordpress.com/2018/05/classifying-phishing-urls-using-recurrent-neural-networks_cameraready.pdf\n",
    "  \n",
    "Para realizar la actividad, solo siga las indicaciones asociadas a cada celda del notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar base de datos y librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize('test')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.word_tokenize(\"example\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_1(text):\n",
    "    if isinstance(text, str): # Asegurarse de que la entrada sea una cadena\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text) # Liempieza de caracteres diferentes a letras, sustitucion por \" \"\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2] # Lematización y eliminación de stopwords y palabras cortas\n",
    "        return ' '.join(tokens)\n",
    "    return '' # Manejar casos donde el texto no es una cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['plot_clean_1'] = dataTraining['plot'].apply(preprocess_text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar y guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=1000)\n",
    "X_dtm_1 = vect.fit_transform(dataTraining['plot_clean_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['genres_list'] = dataTraining['genres'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genre_matrix = mlb.fit_transform(dataTraining['genres_list'])\n",
    "\n",
    "genre_labels = mlb.classes_\n",
    "\n",
    "# Crear nuevo DataFrame con columnas binarias\n",
    "genre_df = pd.DataFrame(genre_matrix, columns=genre_labels)\n",
    "\n",
    "# Unir al dataframe original si lo deseas\n",
    "dataTraining_bin = pd.concat([dataTraining.reset_index(drop=True), genre_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))  # convertir a lista\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(n_jobs=-1, max_iter=1000, random_state=42))\n",
    "clf.fit(X_dtm_1, y_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar modelo a archivo binario .pkl\n",
    "joblib.dump((clf, vect), 'genres_movie.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba: Para probar el modelo vamos a crear una funcion dentor de un .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de ejemplo para predecir\n",
    "X_test = \"As a group of first responders escape the clutches of death, they begin to be killed by increasingly improbable and murderous mishaps.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir función de predicción usando los objetos ya cargados en el notebook\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\CYBER\\Documents\\Maestria MIAD\\Maestria-MIAD\\Machine Learning MIAD\\MIAD_ML_NLP_2025-1\\Entregas_ML_NLP\\Semana 7\\AWS')\n",
    "from Proyecto2_model_deployment_P2 import predict_genre, preprocess_text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text: group first responder escape clutch death begin killed increasingly improbable murderous mishap\n",
      "  (np.int32(0), np.int32(87))\t1\n",
      "  (np.int32(0), np.int32(224))\t1\n",
      "  (np.int32(0), np.int32(281))\t1\n",
      "  (np.int32(0), np.int32(325))\t1\n",
      "  (np.int32(0), np.int32(380))\t1\n",
      "  (np.int32(0), np.int32(473))\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Action</th>\n",
       "      <th>p_Adventure</th>\n",
       "      <th>p_Animation</th>\n",
       "      <th>p_Biography</th>\n",
       "      <th>p_Comedy</th>\n",
       "      <th>p_Crime</th>\n",
       "      <th>p_Documentary</th>\n",
       "      <th>p_Drama</th>\n",
       "      <th>p_Family</th>\n",
       "      <th>p_Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>p_Musical</th>\n",
       "      <th>p_Mystery</th>\n",
       "      <th>p_News</th>\n",
       "      <th>p_Romance</th>\n",
       "      <th>p_Sci-Fi</th>\n",
       "      <th>p_Short</th>\n",
       "      <th>p_Sport</th>\n",
       "      <th>p_Thriller</th>\n",
       "      <th>p_War</th>\n",
       "      <th>p_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385867</td>\n",
       "      <td>0.18569</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.043309</td>\n",
       "      <td>0.153783</td>\n",
       "      <td>0.120446</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.419607</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.106069</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.073108</td>\n",
       "      <td>0.061457</td>\n",
       "      <td>0.036815</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.428163</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>0.018609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Action  p_Adventure  p_Animation  p_Biography  p_Comedy   p_Crime  \\\n",
       "0  0.385867      0.18569     0.001377     0.043309  0.153783  0.120446   \n",
       "\n",
       "   p_Documentary   p_Drama  p_Family  p_Fantasy  ...  p_Musical  p_Mystery  \\\n",
       "0       0.016197  0.419607  0.009604   0.050776  ...   0.002883   0.106069   \n",
       "\n",
       "     p_News  p_Romance  p_Sci-Fi   p_Short   p_Sport  p_Thriller     p_War  \\\n",
       "0  0.000669   0.073108  0.061457  0.036815  0.001902    0.428163  0.033223   \n",
       "\n",
       "   p_Western  \n",
       "0   0.018609  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesar y vectorizar el texto antes de predecir\n",
    "predict_genre(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

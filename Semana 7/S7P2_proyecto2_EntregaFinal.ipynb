{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8f889c",
   "metadata": {},
   "source": [
    "### üìä **Informe: Modelo de Clasificaci√≥n de G√©nero de Pel√≠culas**\n",
    "\n",
    "### üë• Integrantes\n",
    "- Miguel Mateo Sandoval Torres  \n",
    "- Diego Dayan Ni√±o P√©rez  \n",
    "- Camilo Andr√©s Fl√≥rez Esquivel  \n",
    "- Andrea Amariles Escobar  \n",
    "\n",
    "### üìö Curso\n",
    "**Machine Learning y Procesamiento de Lenguaje Natural**\n",
    "\n",
    "### üóìÔ∏è Fecha\n",
    "**Mayo de 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38915ce6",
   "metadata": {},
   "source": [
    "### **Introducci√≥n** <br>\n",
    "<p style=\"text-align: justify;\">\n",
    "Este informe presenta el desarrollo de un modelo de aprendizaje autom√°tico, cuyo objetivo es predecir la probabilidad de que una pel√≠cula pertenezca a un g√©nero en particular. A lo largo del documento se describen las etapas fundamentales del proceso, incluyendo el preprocesamiento de datos, la selecci√≥n y calibraci√≥n del modelo, el entrenamiento y evaluaci√≥n del rendimiento del mismo, al igual que m√©tricas comparativas con versiones alternativas, con el objetivo de evidenciar el proceso de selecci√≥n del modelo final. Finalmente, se presenta el procedimiento de disponibilizaci√≥n del modelo predictivo mediante una API.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd60e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n librer√≠as\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursos\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize('test')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.word_tokenize(\"example\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62090e49",
   "metadata": {},
   "source": [
    "### **Carga y Preprocesamiento de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65855692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)\n",
    "\n",
    "# Visualizaci√≥n datos de entrenamiento\n",
    "dataTraining.head()\n",
    "\n",
    "# Visualizaci√≥n datos de test\n",
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisi√≥n general de los datos\n",
    "nulos = dataTraining.isnull().sum()               # Busqueda de valores nulos\n",
    "duplicados = dataTraining.duplicated().sum()      # Busqueda de valores repetidos\n",
    "titulos_unicos = dataTraining['title'].nunique()  # T√≠tulos √∫nicos de pel√≠culas\n",
    "dataTraining.info()                               # Informaci√≥n general de los datos\n",
    "\n",
    "print(f'Valores nulos:\\n{nulos}')\n",
    "print(f'\\nValores duplicados: {duplicados}')\n",
    "print(f'\\nCantidad de t√≠tulos √∫nicos: {titulos_unicos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversi√≥n de la columna 'genres' de string a lista\n",
    "dataTraining['genres_list'] = dataTraining['genres'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8428bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de pel√≠culas por g√©nero\n",
    "frecuencia_generos = list(chain.from_iterable(dataTraining['genres_list']))\n",
    "conteo_generos = Counter(frecuencia_generos)\n",
    "\n",
    "sorted_genres = conteo_generos.most_common()\n",
    "genres, counts = zip(*sorted_genres)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(genres, counts, color='skyblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('G√©nero')\n",
    "plt.ylabel('Cantidad de pel√≠culas')\n",
    "plt.title('Frecuencia de g√©neros en el dataset')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Paso 5: A√±adir etiquetas a cada barra\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             str(count), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5fac0",
   "metadata": {},
   "source": [
    "A continuaci√≥n, se definen algunas funciones de preporcesamiento de texto que ser√°n aplicadas m√°s adelante sobre los datos de entrada para entrenar los modelos predictivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializaci√≥n de herramientas\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Funci√≥n de preprocesamiento de texto \n",
    "def preprocess_text_1(text):\n",
    "    \"\"\"\n",
    "    Esta es una funci√≥n de preprocesamiento de texto que realiza las siguientes tareas:\n",
    "    - Convierte el texto a min√∫sculas.\n",
    "    - Elimina caracteres no alfab√©ticos.\n",
    "    - Tokeniza el texto en palabras individuales.\n",
    "    - Elimina stopwords y palabras de menos de 3 letras.\n",
    "    - Lematiza cada palabra (reduce a su forma base).\n",
    "    - Devuelve el texto limpio como una cadena de palabras separadas por espacios.\n",
    "\n",
    "    Par√°metros:\n",
    "        text (str): Texto de entrada.\n",
    "\n",
    "    Retorna:\n",
    "        str: Texto preprocesado para an√°lisis o modelado.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        return ' '.join(tokens)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de procesamiento de texto con BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    \"\"\"\n",
    "    Genera un embedding num√©rico para un texto utilizando BERT.\n",
    "    - Tokeniza el texto de entrada usando el tokenizer de BERT preentrenado.\n",
    "    - Procesa el texto con el modelo BERT para obtener los embeddings de cada token.\n",
    "    - Calcula el promedio de los embeddings de todos los tokens para obtener una √∫nica representaci√≥n del texto.\n",
    "    - Devuelve el embedding como un vector de NumPy.\n",
    "\n",
    "    Par√°metros:\n",
    "        text (str): Texto de entrada a convertir en embedding.\n",
    "\n",
    "    Retorna:\n",
    "        np.ndarray: Vector num√©rico que representa el significado del texto seg√∫n BERT.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        outputs = bert_model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        embedding = last_hidden_state.mean(dim=1)\n",
    "        return embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b2089",
   "metadata": {},
   "source": [
    "### **Entrenamiento de Modelos Predictivos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80fe14",
   "metadata": {},
   "source": [
    "Se entrenaron y calibraron diferentes modelos predictivos. A continuaci√≥n se presentan tres modelos a partir de los cuales se eligi√≥ el que present√≥ el mejor desempe√±o:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63e70c",
   "metadata": {},
   "source": [
    "#### Modelo 1: Regresi√≥n Log√≠stica con NLTK y CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec867c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar preprocesamiento a los datos de entrenamiento (limpieza, tokenizaci√≥n y lematizaci√≥n)\n",
    "dataTraining['plot_clean_1'] = dataTraining['plot'].apply(preprocess_text_1)\n",
    "dataTesting['plot_clean_1'] = dataTesting['plot'].apply(preprocess_text_1)\n",
    "\n",
    "# Aplicaci√≥n de CountVectorizer para convertir texto en matriz num√©rica\n",
    "vect = CountVectorizer(max_features=1000)\n",
    "X_dtm_1 = vect.fit_transform(dataTraining['plot_clean_1'])\n",
    "X_dtm_1.shape\n",
    "\n",
    "# Codificaci√≥n de etiquetas de salida (g√©neros)\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7023e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n de los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm_1, y_genres, \n",
    "                                                                  test_size=0.33, \n",
    "                                                                  random_state=42\n",
    "                                                                  )\n",
    "# Clasificador Regresi√≥n Log√≠stica y One-vs-Rest\n",
    "clf = OneVsRestClassifier(LogisticRegression(n_jobs=-1, max_iter=1000, random_state=42))\n",
    "clf.fit(X_train, y_train_genres)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_genres = clf.predict_proba(X_test)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "AUC_LogReg_nltk = roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c31d4",
   "metadata": {},
   "source": [
    "#### Modelo 2: Regresi√≥n Log√≠stica con Embeddings BERT y One-vs-Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21763317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['plot_clean_bert'] = dataTraining['plot'].apply(get_bert_embedding)\n",
    "dataTesting['plot_clean_bert'] = dataTesting['plot'].apply(get_bert_embedding)\n",
    "\n",
    "X_bert = np.vstack(dataTraining['plot_clean_bert'].values)\n",
    "\n",
    "# Divisi√≥n de los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_bert, y_genres, \n",
    "                                                                  test_size=0.33, \n",
    "                                                                  random_state=42\n",
    "                                                                  )\n",
    "# # Clasificador Regresi√≥n Log√≠stica y One-vs-Rest\n",
    "clf = OneVsRestClassifier(LogisticRegression(n_jobs=-1, max_iter=1000, random_state=42))\n",
    "clf.fit(X_train, y_train_genres)\n",
    "\n",
    "# Predicci√≥n del modelo de clasificaci√≥n\n",
    "y_pred_genres = clf.predict_proba(X_test)\n",
    "\n",
    "# Impresi√≥n del desempe√±o del modelo\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7d149",
   "metadata": {},
   "source": [
    "#### Modelo 3: Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0c623",
   "metadata": {},
   "source": [
    "### **Comparaci√≥n y Selecci√≥n del Mejor Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6eef1",
   "metadata": {},
   "source": [
    "### **Disponibilizaci√≥n del Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0d4ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
